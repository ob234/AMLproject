{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90efd1b7",
   "metadata": {
    "id": "90efd1b7"
   },
   "source": [
    "# Yelp Analysis\n",
    "### 11/10/2022\n",
    "In this table I'll join the tables in the Yelp academic dataset. \\\n",
    "There are 5 tables, we'll import all 5 Json files here.\\\n",
    "[Documentation for Dataset](https://www.yelp.com/dataset/documentation/main)\n",
    "\n",
    "\n",
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195224ea",
   "metadata": {
    "executionInfo": {
     "elapsed": 1871,
     "status": "ok",
     "timestamp": 1668704324502,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "195224ea"
   },
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4fa541",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "error",
     "timestamp": 1668704324503,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "2d4fa541",
    "outputId": "4d96ec84-5fa3-4cfa-fa42-23e19a949fbe",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a2095e6e6a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# importing business\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbusiness_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/yelp_business_restaurants.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# remove columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# we remove the average restaurant rating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/yelp_business_restaurants.csv'"
     ]
    }
   ],
   "source": [
    "# importing business\n",
    "business_df = pd.read_csv(\"Data/yelp_business_restaurants.csv\")\n",
    "\n",
    "# remove columns\n",
    "# we remove the average restaurant rating\n",
    "# we only care for the stars in the review_df (map text to rating)\n",
    "business_df.drop(['stars'], axis = 1, inplace = True)\n",
    "business_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "# get only businesses who are restaurants\n",
    "business_df = business_df[business_df['is_restaurant'] == True]\n",
    "\n",
    "# rename 'name' to 'business_name'\n",
    "business_df.columns = business_df.columns.str.replace('name', 'business_name')\n",
    "\n",
    "# rename 'review_count' to 'business_review_count'\n",
    "business_df.columns = business_df.columns.str.replace('review_count', 'business_review_count')\n",
    "\n",
    "# snapshot\n",
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8945b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1668704324503,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "fd8945b6"
   },
   "outputs": [],
   "source": [
    "# importing review\n",
    "data_file = open(\"Data/yelp_academic_dataset_review.json\", 'r', encoding='utf8')\n",
    "data = []\n",
    "for line in data_file:\n",
    "    data.append(json.loads(line))\n",
    "review_df = pd.DataFrame(data)\n",
    "data_file.close()\n",
    "\n",
    "# snapshot\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664df041",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1668704324504,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "664df041"
   },
   "outputs": [],
   "source": [
    "# importing user\n",
    "data_file = open(\"Data/yelp_academic_dataset_user.json\", 'r', encoding='utf8')\n",
    "data = []\n",
    "for line in data_file:\n",
    "    data.append(json.loads(line))\n",
    "user_df = pd.DataFrame(data)\n",
    "data_file.close()\n",
    "\n",
    "# snapshot\n",
    "user_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec13436",
   "metadata": {
    "id": "cec13436"
   },
   "source": [
    "## Transform\n",
    "\n",
    "1. Filter for businesses that are food or restaurant related (use `yelp_categories` dataset)\n",
    "2. Filter for users who have 40 or more reviews.\n",
    "3. Filter for businesses who have 10 or more reviews.\n",
    "4. Filter for businesses in PA & NJ.\n",
    "5. Filter for reviews in PA & NJ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78232e75",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1668704324504,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "78232e75"
   },
   "outputs": [],
   "source": [
    "sns.histplot(user_df['review_count'], binwidth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75c03d",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1668704324504,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "df75c03d"
   },
   "outputs": [],
   "source": [
    "user_df['review_count'].quantile([0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867976fa",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324505,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "867976fa"
   },
   "outputs": [],
   "source": [
    "# 1. Filter for users who have more than 1 reviews\n",
    "#    Filter for users who have less than 92 reviews\n",
    "user_df = user_df[(user_df['review_count'] > 1) |\\\n",
    "                  (user_df['review_count'] < 92)]\n",
    "print(len(user_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880e7bd",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324505,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "7880e7bd"
   },
   "outputs": [],
   "source": [
    "# 2. Outliers in review count for businesses\n",
    "business_df['business_review_count'].quantile([0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efc7b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324505,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "95efc7b6"
   },
   "outputs": [],
   "source": [
    "# 2. Filter for businesses who have more than 11 reviews\n",
    "#    Filter for businesses who have less than 290 reviews\n",
    "business_df = business_df[(business_df['business_review_count'] >= 11) |\\\n",
    "                          (business_df['business_review_count'] <= 290)]\n",
    "\n",
    "print(len(business_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbcba5c",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324505,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "dfbcba5c"
   },
   "outputs": [],
   "source": [
    "# get count of reviews by state\n",
    "print(business_df.groupby(['state'])['business_review_count'].agg(np.size).sort_values())\n",
    "\n",
    "# count plot of businesses in different states\n",
    "sns.set(rc={\"figure.figsize\":(10, 6)}) #width=10, #height=6\n",
    "sns.histplot(business_df['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579cf55",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324506,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "a579cf55"
   },
   "outputs": [],
   "source": [
    "# 3. Filter for businesses in CA\n",
    "business_df = business_df[business_df['state'].isin(['CA'])]\n",
    "\n",
    "# check\n",
    "print(business_df.state.unique())\n",
    "\n",
    "# snapshot\n",
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a03ba",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324506,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "d52a03ba"
   },
   "outputs": [],
   "source": [
    "# 4. Merging reviews with users\n",
    "master_df = review_df.merge(user_df, how='inner', on=\"user_id\")\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f029157",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1668704324506,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "3f029157"
   },
   "outputs": [],
   "source": [
    "# 4. Merging master with users\n",
    "master_df = master_df.merge(business_df, how='inner', on=\"business_id\")\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b109b",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1668704324507,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "1e3b109b"
   },
   "outputs": [],
   "source": [
    "# summary stats\n",
    "print(\"After data processing ...\")\n",
    "print(f\"# of Unique Reviews: {'{:,}'.format(len(master_df.review_id.unique()))}\")\n",
    "print(f\"# of Unique Users: {'{:,}'.format(len(master_df.user_id.unique()))}\")\n",
    "print(f\"# of Unique Businesses: {'{:,}'.format(len(master_df.business_id.unique()))}\")\n",
    "print(\"===\" * 10)\n",
    "print()\n",
    "\n",
    "print(\"Regarding users ...\")\n",
    "print(f\"Mean review count per user: {round(master_df.user_id.value_counts().mean(), 4)}\")\n",
    "print(f\"Median review count per user: {round(master_df.user_id.value_counts().median(), 4)}\")\n",
    "print(f\"Variance of review counts per user: {round(master_df.user_id.value_counts().var(), 4)}\")\n",
    "q3, q1 = np.percentile(master_df.user_id.value_counts(), [75, 25])\n",
    "print(f\"IQR of review counts per user: {round(q3 - q1, 4)}\")\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "print(\"Regarding stars per user ...\")\n",
    "print(\"Mean stars per user: \" + str(round(master_df.groupby(\"user_id\")['stars'].mean().mean(), 4)))\n",
    "print(\"Median stars per user: \" + str(round(master_df.groupby(\"user_id\")['stars'].mean().median(), 4)))\n",
    "print(\"Variance of stars per user: \" + str(round(master_df.groupby(\"user_id\")['stars'].mean().var(), 4)))\n",
    "q3, q1 = np.percentile(master_df.groupby(\"user_id\")['stars'].mean(), [75, 25])\n",
    "print(f\"IQR of stars per user: {round(q3 - q1, 4)}\")\n",
    "print(\"===\" * 10)\n",
    "print()\n",
    "\n",
    "print(\"Regarding businesses ...\")\n",
    "print(f\"Mean review count per business: {round(master_df.business_id.value_counts().mean(), 4)}\")\n",
    "print(f\"Median review count per business: {round(master_df.business_id.value_counts().median(), 4)}\")\n",
    "print(f\"Variance of review counts per business: {round(master_df.business_id.value_counts().var(), 4)}\")\n",
    "q3, q1 = np.percentile(master_df.business_id.value_counts(), [75, 25])\n",
    "print(f\"IQR of review counts per business: {round(q3 - q1, 4)}\")\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "print(\"Regarding stars per business ...\")\n",
    "print(\"Mean stars per business: \" + str(round(master_df.groupby(\"business_id\")['stars'].mean().mean(), 4)))\n",
    "print(\"Median stars per business: \" + str(round(master_df.groupby(\"business_id\")['stars'].mean().median(), 4)))\n",
    "print(\"Variance of stars per business: \" + str(round(master_df.groupby(\"business_id\")['stars'].mean().var(), 4)))\n",
    "q3, q1 = np.percentile(master_df.groupby(\"business_id\")['stars'].mean(), [75, 25])\n",
    "print(f\"IQR of mean stars per business: {round(q3 - q1, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfe03e",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1668704324507,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "59bfe03e"
   },
   "outputs": [],
   "source": [
    "# output final dataset\n",
    "master_df.to_csv(\"master_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5237b",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1668704324507,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "1ba5237b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import master df\n",
    "master_df = pd.read_csv(\"master_df.csv\")\n",
    "\n",
    "# snapshot\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae717e",
   "metadata": {
    "id": "50ae717e"
   },
   "source": [
    "## California Restaurants Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0f922",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324508,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "f8c0f922"
   },
   "outputs": [],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ed0a9",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324508,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "1a2ed0a9"
   },
   "outputs": [],
   "source": [
    "# number of restaurants per city\n",
    "master_df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac625a",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1668704324508,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "dfac625a"
   },
   "outputs": [],
   "source": [
    "# mean restaurant stars per city\n",
    "master_df.groupby(['city'])['stars'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84b87a",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1668704324508,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "3a84b87a"
   },
   "outputs": [],
   "source": [
    "# mean restaurant reviews per city\n",
    "master_df.groupby(['city'])['business_review_count'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e1955",
   "metadata": {
    "id": "183e1955"
   },
   "source": [
    "## Cleaning text data\n",
    "\n",
    "Per line of text:\n",
    "1. Remove punctuation\n",
    "2. Tokenize\n",
    "3. Remove stop words\n",
    "4. Stem words\n",
    "5. Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2c029",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324509,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "cac2c029"
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "master_df = master_df[['stars', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a3ea5",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324509,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "3d1a3ea5"
   },
   "outputs": [],
   "source": [
    "# import dependencies to clean text\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "stopword = nltk.corpus.stopwords.words('english') # english stopwords\n",
    "ps = nltk.PorterStemmer() # stem english words\n",
    "wn = nltk.WordNetLemmatizer() # lemmatize english words\n",
    "\n",
    "# function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "    Removing punctuation.\n",
    "    \"\"\"\n",
    "    text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_no_punct\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizing text.\n",
    "    \"\"\"\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stop_words(tokenized_text):\n",
    "    \"\"\"\n",
    "    Removing stop words.\n",
    "    \"\"\"\n",
    "    text = \" \".join([word for word in tokenized_text if word not in stopword])\n",
    "    return text\n",
    "\n",
    "def text_stemmer(tokenized_text):\n",
    "    \"\"\"\n",
    "    Reduce words to its stemmed form.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ps.stem(word) for word in tokenized_text])\n",
    "    return text\n",
    "    \n",
    "def text_lemmatize(tokenized_text):\n",
    "    \"\"\"\n",
    "    Reduce words to their root form.\n",
    "    \"\"\"\n",
    "    text = \"\".join([wn.lemmatize(word) for word in tokenized_text])\n",
    "    return text\n",
    "\n",
    "def clean_text_col(text_col):\n",
    "    \"\"\"\n",
    "    Apply text cleaning functions to Pandas Series\n",
    "    \"\"\"\n",
    "    text_col_punc = text_col.apply(lambda x: text_lemmatize(\\\n",
    "                                             text_stemmer(\\\n",
    "                                             remove_stop_words(\\\n",
    "                                             tokenize(\n",
    "                                             remove_punct(x.lower())\\\n",
    "                                             )))))\n",
    "    return text_col_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8e567",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324509,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "26e8e567"
   },
   "outputs": [],
   "source": [
    "# cleaning text\n",
    "X = clean_text_col(master_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcef87",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324509,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "8efcef87"
   },
   "outputs": [],
   "source": [
    "# concat text with target\n",
    "cleaned_text_df = pd.concat([master_df[['stars']], X], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f546f",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1668704324509,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "071f546f"
   },
   "outputs": [],
   "source": [
    "# output to csv\n",
    "pd.concat([master_df[['stars']], X], axis = 1).to_csv(\"cleaned_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d60ea",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324510,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "a29d60ea"
   },
   "outputs": [],
   "source": [
    "# import cleaned text\n",
    "cleaned_text_df = pd.read_csv(\"cleaned_text.csv\").drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "# text\n",
    "X = cleaned_text_df['text']\n",
    "\n",
    "# snapshot\n",
    "cleaned_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6cd641",
   "metadata": {
    "id": "4d6cd641"
   },
   "source": [
    "## Vectorize Text Data\n",
    "\n",
    "Now we encode text as integers to create feature vectors. We will take three approaches to vectorizing the data.\n",
    "\n",
    "**Bag of Words**: This described the presence of words within the text data. The algorithm gives a 1 if the word is present in the sentence, and a 0 if abscent.\n",
    "\n",
    "**N-Gram**: N-grams are a combination of adjacent words or letters of length `n`. We will use bigrams.\n",
    "\n",
    "**TF-IDF**: Term frequency-inverse document frequency defines the proportion of times a word appears in a document over the number of times that same word appears in all other documents.\n",
    "\n",
    "Note: `CountVectorizer +  TfidfTransformer  = TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff8e4d",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324510,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "d0ff8e4d"
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize vectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# fit tf-idf on cleaned text\n",
    "X_tfidf = tfidf_vect.fit_transform(X)\n",
    "\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4d30e",
   "metadata": {
    "id": "12f4d30e"
   },
   "source": [
    "## Truncated SVD to visualize text in 2 Dimensions against Target\n",
    "\n",
    "For a PCA, even if the input is a sparse matrix, the output is not. PCA(X) is SVD(X - mean(X)). As of now, there is no workaround for this in SKLearn. So we'll use Truncated SVD as an alternative. The singular-value decomposition/ SVD is a dimension reduction technique for matrices that reduces the matrix into its component to simplify the calculation. \n",
    "\n",
    "SVD is a popular method for dimensionality reduction. However, it works better with sparse data. This is because the estimator does not center the data before computing the singular value decomposition. Here sparse data refers to the data with many zero values. \n",
    "\n",
    "Because we are working on a TF-IDF matrix, this is known as Latent Semantic Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf58608",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324510,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "caf58608"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# initialize model\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# fit model\n",
    "svd_Components = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# cast as df\n",
    "svd_df = pd.DataFrame(data = svd_Components,\n",
    "                           columns = ['SVD1',\n",
    "                                      'SVD2'])\n",
    "\n",
    "# concatenate stars\n",
    "svd_df = pd.concat([svd_df, master_df[['stars']]], axis = 1)\n",
    "\n",
    "# Percentage of variance explained by each of the selected components.\n",
    "print(f\"Explained variance ratio: {str(round(svd.explained_variance_ratio_.sum(), 10))}\")\n",
    "\n",
    "# snapshot\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525d5f8",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324510,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "7525d5f8"
   },
   "outputs": [],
   "source": [
    "# plot 2D SVD\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.scatterplot(data = svd_df,\n",
    "                x = \"SVD1\",\n",
    "                y = \"SVD2\",\n",
    "                hue = \"stars\",\n",
    "                palette = sns.color_palette(\"hls\", 5),\n",
    "                alpha = 1)\n",
    "plt.xlabel('SVD1', fontsize = 15)\n",
    "plt.ylabel('SVD2', fontsize = 15)\n",
    "plt.title('2 Component Truncated SVD for CA Yelp Reviews', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021d8e9",
   "metadata": {
    "id": "c021d8e9"
   },
   "source": [
    "## Truncated SVD to visualize text in 3 Dimensions against Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523216c",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324511,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "c523216c"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# initialize model\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "\n",
    "# fit model\n",
    "svd_Components = svd.fit_transform(X_tfidf)\n",
    "\n",
    "# cast as df\n",
    "svd_df_3d = pd.DataFrame(data = svd_Components,\n",
    "                           columns = ['SVD1',\n",
    "                                      'SVD2',\n",
    "                                      'SVD3'])\n",
    "\n",
    "# concatenate stars\n",
    "svd_df_3d = pd.concat([svd_df_3d, master_df[['stars']]], axis = 1)\n",
    "\n",
    "# Percentage of variance explained by each of the selected components.\n",
    "print(f\"Explained variance ratio: {str(round(svd.explained_variance_ratio_.sum(), 10))}\")\n",
    "\n",
    "# snapshot\n",
    "svd_df_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221ce41",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324511,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "a221ce41",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "svd_df_3d['stars'] = svd_df_3d['stars'].astype('str')\n",
    "\n",
    "fig = px.scatter_3d(svd_df_3d, x='SVD3', y='SVD2', z='SVD1',\n",
    "                    color='stars', opacity = 0.8)\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e309e8",
   "metadata": {
    "id": "41e309e8"
   },
   "source": [
    "## Naive Bayes Stars Classifier\n",
    "\n",
    "Now we fit a Naive Bayes classifier on the TF-IDF matrix of vectorized words to predict stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ca0cb",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324511,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "eb8ca0cb"
   },
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07b8ac",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324511,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "aa07b8ac"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, master_df['stars'], test_size=0.20, random_state=420)\n",
    "\n",
    "# train validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.25, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5317688",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1668704324511,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "c5317688"
   },
   "outputs": [],
   "source": [
    "# initialize MNB and fit\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# in-sample predicted\n",
    "in_sample_pred = clf.predict(X_train)\n",
    "\n",
    "# OOS predicted\n",
    "oos_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620bfc4a",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1668704324512,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "620bfc4a"
   },
   "outputs": [],
   "source": [
    "# in sample performance metrics\n",
    "print(\"In Sample Classification Report\")\n",
    "print(classification_report(y_train, in_sample_pred, target_names=master_df['stars'].unique().astype(\"str\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0eb292",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1668704324512,
     "user": {
      "displayName": "Amir ElTabakh",
      "userId": "10511769735678863677"
     },
     "user_tz": 300
    },
    "id": "0e0eb292"
   },
   "outputs": [],
   "source": [
    "# in sample performance metrics\n",
    "print(\"Out of Sample Classification Report\")\n",
    "print(classification_report(y_test, oos_pred, target_names=master_df['stars'].unique().astype(\"str\")))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
